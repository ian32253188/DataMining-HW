{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 資料挖掘作業 2：吸菸預測模型\n",
    "\n",
    "本專案透過多個機器學習模型的集成方法來預測吸菸狀態。我們使用了三種強大的梯度提升樹模型：XGBoost、LightGBM 和 CatBoost，並透過 Optuna 進行超參數優化，最後根據驗證集上的 AUC 得分進行加權集成。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 導入所需套件\n",
    "\n",
    "首先導入所有需要使用的套件和函式庫。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T06:50:10.903790Z",
     "iopub.status.busy": "2025-04-15T06:50:10.903559Z",
     "iopub.status.idle": "2025-04-15T06:50:26.362879Z",
     "shell.execute_reply": "2025-04-15T06:50:26.362289Z",
     "shell.execute_reply.started": "2025-04-15T06:50:10.903771Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, PowerTransformer, MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from datetime import datetime\n",
    "import optuna\n",
    "import shap\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 資料載入與探索\n",
    "\n",
    "在這個部分，我們載入訓練和測試資料集，並進行初步的資料探索。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T06:50:26.365121Z",
     "iopub.status.busy": "2025-04-15T06:50:26.364624Z",
     "iopub.status.idle": "2025-04-15T06:50:26.534224Z",
     "shell.execute_reply": "2025-04-15T06:50:26.533609Z",
     "shell.execute_reply.started": "2025-04-15T06:50:26.365103Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 載入資料集\n",
    "train = pd.read_csv(\"/kaggle/input/hw2-data/test.csv\")\n",
    "test = pd.read_csv(\"/kaggle/input/hw2-data/train.csv\")\n",
    "sample_submission = pd.read_csv(\"/kaggle/input/hw2-data/sample_submission.csv\")\n",
    "\n",
    "# 顯示訓練集的基本資訊\n",
    "print(f\"訓練集形狀: {train.shape}\")\n",
    "print(f\"測試集形狀: {test.shape}\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 資料前處理\n",
    "\n",
    "資料前處理是機器學習流程中非常重要的一環。在這一部分，我們將執行以下步驟：\n",
    "1. 合併訓練集和測試集以進行一致的特徵工程\n",
    "2. 處理欄位名稱（替換空格為下劃線）\n",
    "3. 辨識並分類特徵（類別型和數值型）\n",
    "4. 進行缺失值處理和特徵轉換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T06:50:26.535355Z",
     "iopub.status.busy": "2025-04-15T06:50:26.535058Z",
     "iopub.status.idle": "2025-04-15T06:50:26.563925Z",
     "shell.execute_reply": "2025-04-15T06:50:26.563176Z",
     "shell.execute_reply.started": "2025-04-15T06:50:26.535322Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 合併訓練集和測試集以統一進行特徵處理\n",
    "test['smoking'] = np.nan\n",
    "data = pd.concat([train, test], ignore_index=True)\n",
    "\n",
    "# 3. Feature Engineering\n",
    "\n",
    "data['BMI'] = data['weight(kg)'] / (data['height(cm)'] / 100) ** 2\n",
    "# data['LDL_to_HDL'] = data['LDL'] / (data['HDL'] + 1e-5)\n",
    "# data['waist_BMI_ratio'] = data['waist(cm)'] / data['BMI']\n",
    "# data['liver_mean'] = data[['AST', 'ALT', 'Gtp']].mean(axis=1)\n",
    "\n",
    "# 定義類別型和數值型特徵\n",
    "categorical_columns = ['hearing(left)', 'hearing(right)', 'Urine protein', 'dental caries']\n",
    "numerical_columns = [col for col in data.columns if col not in categorical_columns + ['smoking', 'id']\n",
    "                     and data[col].dtype in ['float64', 'int64']]\n",
    "\n",
    "# 顯示分類後的特徵數量\n",
    "print(f\"類別型特徵數量: {len(categorical_columns)}\")\n",
    "print(f\"數值型特徵數量: {len(numerical_columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 缺失值處理與特徵轉換\n",
    "\n",
    "我們使用以下方法處理資料：\n",
    "1. 使用 SimpleImputer 填補缺失值（以中位數填補）\n",
    "2. 應用 Yeo-Johnson 變換來處理偏態分佈\n",
    "3. 使用 MinMaxScaler 將數值特徵縮放到相同範圍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T06:50:26.565068Z",
     "iopub.status.busy": "2025-04-15T06:50:26.564727Z",
     "iopub.status.idle": "2025-04-15T06:50:26.852293Z",
     "shell.execute_reply": "2025-04-15T06:50:26.851546Z",
     "shell.execute_reply.started": "2025-04-15T06:50:26.565047Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 應用 Power Transform 處理偏態分佈\n",
    "power_transformer = PowerTransformer(method='yeo-johnson')\n",
    "data[numerical_columns] = power_transformer.fit_transform(data[numerical_columns])\n",
    "\n",
    "# 特徵縮放\n",
    "scaler = MinMaxScaler()\n",
    "data[numerical_columns] = scaler.fit_transform(data[numerical_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 類別型特徵編碼與特徵工程\n",
    "\n",
    "在這個部分，我們將：\n",
    "1. 對類別型特徵進行 One-Hot 編碼\n",
    "2. 應用 KMeans 聚類作為特徵工程的一部分，創建新的聚類特徵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T06:50:26.853307Z",
     "iopub.status.busy": "2025-04-15T06:50:26.853077Z",
     "iopub.status.idle": "2025-04-15T06:50:26.858311Z",
     "shell.execute_reply": "2025-04-15T06:50:26.857595Z",
     "shell.execute_reply.started": "2025-04-15T06:50:26.853281Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T06:50:26.859424Z",
     "iopub.status.busy": "2025-04-15T06:50:26.859148Z",
     "iopub.status.idle": "2025-04-15T06:50:26.902620Z",
     "shell.execute_reply": "2025-04-15T06:50:26.902137Z",
     "shell.execute_reply.started": "2025-04-15T06:50:26.859400Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# One-Hot 編碼\n",
    "encoder = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "encoded = encoder.fit_transform(data[categorical_columns])\n",
    "encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out(categorical_columns), index=data.index)\n",
    "data = data.drop(columns=categorical_columns)\n",
    "data = pd.concat([data, encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T06:50:26.904562Z",
     "iopub.status.busy": "2025-04-15T06:50:26.904363Z",
     "iopub.status.idle": "2025-04-15T06:50:27.384134Z",
     "shell.execute_reply": "2025-04-15T06:50:27.383501Z",
     "shell.execute_reply.started": "2025-04-15T06:50:26.904546Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 使用 KMeans 進行聚類特徵工程\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "data['kmeans_cluster'] = kmeans.fit_predict(data[numerical_columns])\n",
    "\n",
    "# 檢視特徵工程後的資料\n",
    "print(f\"處理後的特徵數量: {data.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T06:50:27.384967Z",
     "iopub.status.busy": "2025-04-15T06:50:27.384765Z",
     "iopub.status.idle": "2025-04-15T06:50:27.389619Z",
     "shell.execute_reply": "2025-04-15T06:50:27.388888Z",
     "shell.execute_reply.started": "2025-04-15T06:50:27.384949Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 資料分割\n",
    "\n",
    "將資料分割為訓練集、驗證集和測試集。這是模型訓練和評估的重要步驟。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-04-15T06:50:27.390790Z",
     "iopub.status.busy": "2025-04-15T06:50:27.390463Z",
     "iopub.status.idle": "2025-04-15T06:50:27.478596Z",
     "shell.execute_reply": "2025-04-15T06:50:27.477695Z",
     "shell.execute_reply.started": "2025-04-15T06:50:27.390761Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. 讀取資料（根據你 Kaggle 加入的資料集名稱）\n",
    "train = pd.read_csv(\"/kaggle/input/hw2-data/train.csv\")\n",
    "test = pd.read_csv(\"/kaggle/input/hw2-data/test.csv\")\n",
    "\n",
    "# 2. 標記訓練集長度，用來之後切分\n",
    "train_length = len(train)\n",
    "\n",
    "# 3. 合併資料\n",
    "data = pd.concat([train, test], ignore_index=True)\n",
    "\n",
    "# 4. 刪除不需要的特徵\n",
    "features_to_drop = ['hearing(left)_2.0', 'hearing(right)_2.0', 'eyesight(left)', 'eyesight(right)','BMI','Urine protein_2.0','Urine protein_3.0', 'Urine protein_4.0', 'Urine protein_5.0','kmeans_cluster']\n",
    "data = data.drop(columns=features_to_drop, errors='ignore')\n",
    "\n",
    "# 5. 切分訓練與測試資料\n",
    "X = data.iloc[:train_length].drop(columns=['smoking', 'id'], errors='ignore')\n",
    "y = data.iloc[:train_length]['smoking'].astype(int)\n",
    "X_test = data.iloc[train_length:].drop(columns=['smoking', 'id'], errors='ignore')\n",
    "\n",
    "# 6. 拆分訓練與驗證集\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.1, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 7. 顯示形狀確認\n",
    "print(f\"訓練集形狀: {X_train.shape}\")\n",
    "print(f\"驗證集形狀: {X_val.shape}\")\n",
    "print(f\"測試集形狀: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 模型超參數優化與訓練\n",
    "\n",
    "在這個部分，我們將使用 Optuna 來為三種不同的梯度提升樹模型尋找最佳的超參數。超參數優化是提高模型性能的關鍵步驟。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 XGBoost 模型優化與訓練\n",
    "\n",
    "XGBoost 是一種高效能的梯度提升樹實現，特別適合結構化/表格式資料。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T06:50:27.480001Z",
     "iopub.status.busy": "2025-04-15T06:50:27.479693Z",
     "iopub.status.idle": "2025-04-15T06:51:59.334417Z",
     "shell.execute_reply": "2025-04-15T06:51:59.333800Z",
     "shell.execute_reply.started": "2025-04-15T06:50:27.479976Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ✅ 修正後的 XGBoost + Optuna + SHAP 流程\n",
    "# 1️⃣ 避免資料洩漏：CV 只使用 X_train\n",
    "# 3️⃣ n_estimators 上限擴至 1000\n",
    "# 4️⃣ 保留 X_val 作為驗證集\n",
    "# ✅ 使用 pandas DataFrame\n",
    "\n",
    "import optuna\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# # 假設 data, train 已定義並預處理\n",
    "# train_length = len(train)\n",
    "# X = data.iloc[:train_length].drop(columns=['smoking', 'id'], errors='ignore')\n",
    "# X_test = data.iloc[train_length:].drop(columns=['smoking', 'id'], errors='ignore')\n",
    "# y = data.iloc[:train_length]['smoking'].astype(int)\n",
    "\n",
    "# # ✅ 分割資料，保留 X_val 為未見驗證集\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, stratify=y, random_state=42)\n",
    "\n",
    "#-----------------------------\n",
    "# 1. Define Optuna objective function\n",
    "#-----------------------------\n",
    "def objective_xgb(trial):\n",
    "    params = {\n",
    "        'n_estimators': 500,  # \n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 6),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 5, 15),\n",
    "        'gamma': trial.suggest_float('gamma', 1, 10),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.5, 10.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1.0, 10.0),\n",
    "        'tree_method': 'hist',\n",
    "        'device': 'cuda',\n",
    "        'eval_metric': 'auc'\n",
    "    }\n",
    "\n",
    "    aucs = []\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    for train_idx, val_idx in skf.split(X_train, y_train):  # ✅ 用 X_train 避免資料洩漏\n",
    "        X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        model = xgb.XGBClassifier(**params)\n",
    "        model.fit(\n",
    "            X_fold_train,\n",
    "            y_fold_train,\n",
    "            eval_set=[(X_fold_val, y_fold_val)],\n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        preds = model.predict_proba(X_fold_val)[:, 1]\n",
    "        aucs.append(roc_auc_score(y_fold_val, preds))\n",
    "\n",
    "    mean_auc = np.mean(aucs)\n",
    "    trial.set_user_attr(\"params\", params)\n",
    "    trial.set_user_attr(\"mean_auc\", mean_auc)\n",
    "    return mean_auc\n",
    "\n",
    "#-----------------------------\n",
    "# 2. Run Optuna optimization\n",
    "#-----------------------------\n",
    "study_xgb = optuna.create_study(direction='maximize')\n",
    "study_xgb.optimize(objective_xgb, n_trials=30)\n",
    "\n",
    "print(\"Best XGBoost Parameters:\")\n",
    "print(study_xgb.best_params)\n",
    "print(f\"Best AUC: {study_xgb.best_value:.4f}\")\n",
    "\n",
    "#-----------------------------\n",
    "# 3. Train Final Model\n",
    "#-----------------------------\n",
    "\n",
    "best_xgb = xgb.XGBClassifier(\n",
    "    **study_xgb.best_params,\n",
    "    n_estimators=500,\n",
    "    tree_method='hist',\n",
    "    device='cuda',\n",
    "    eval_metric='auc',\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "best_xgb.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False  # ❌ 拿掉 eval_metric 避免衝突\n",
    ")\n",
    "\n",
    "\n",
    "#-----------------------------\n",
    "# 4. Draw learning curve\n",
    "#-----------------------------\n",
    "results = best_xgb.evals_result()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(results['validation_0']['auc'], label='Train AUC')\n",
    "plt.plot(results['validation_1']['auc'], label='Validation AUC')\n",
    "plt.xlabel('Boosting Round')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.title('XGBoost Learning Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#-----------------------------\n",
    "# 5. Predict on validation set & draw confusion matrix\n",
    "#-----------------------------\n",
    "xgb_preds = best_xgb.predict_proba(X_val)\n",
    "xgb_labels = np.argmax(xgb_preds, axis=1)\n",
    "cm = confusion_matrix(y_val, xgb_labels)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('XGBoost Confusion Matrix (Validation Set)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#-----------------------------\n",
    "# 6. Feature Importance Plot\n",
    "#-----------------------------\n",
    "plt.figure(figsize=(10, 6))\n",
    "xgb.plot_importance(best_xgb, max_num_features=15, importance_type='gain', height=0.5)\n",
    "plt.title(\"XGBoost Feature Importance (Top 15, by Gain)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#-----------------------------\n",
    "# 7. SHAP Analysis (Global Explanation)\n",
    "#-----------------------------\n",
    "explainer = shap.Explainer(best_xgb, X_train, feature_names=X_train.columns)\n",
    "shap_values = explainer(X_val)\n",
    "shap.plots.beeswarm(shap_values, max_display=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 LightGBM 模型優化與訓練\n",
    "\n",
    "LightGBM 是一個高效、低記憶體佔用的梯度提升框架，使用基於直方圖的分割尋找策略，適合大型資料集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T06:51:59.335939Z",
     "iopub.status.busy": "2025-04-15T06:51:59.335354Z",
     "iopub.status.idle": "2025-04-15T06:53:30.760024Z",
     "shell.execute_reply": "2025-04-15T06:53:30.759170Z",
     "shell.execute_reply.started": "2025-04-15T06:51:59.335906Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ✅ 修正後的 LightGBM + Optuna + SHAP 流程\n",
    "# 1️⃣ 避免資料洩漏（只使用 X_train 做 CV）\n",
    "# 2️⃣ 使用 pandas DataFrame（非 .values）\n",
    "# 3️⃣ n_estimators 上限提至 1000\n",
    "# 4️⃣ 保留 X_val 做最終驗證評估\n",
    "\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "import shap\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 假設 data, train 已定義\n",
    "# features_to_drop = [...]\n",
    "# data = data.drop(columns=features_to_drop, errors='ignore')\n",
    "# train_length = len(train)\n",
    "# X = data.iloc[:train_length].drop(columns=['smoking', 'id'], errors='ignore')\n",
    "# X_test = data.iloc[train_length:].drop(columns=['smoking', 'id'], errors='ignore')\n",
    "# y = data.iloc[:train_length]['smoking'].astype(int)\n",
    "\n",
    "# # ✅ 分割訓練與驗證集，保留 X_val 評估用\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, stratify=y, random_state=42)\n",
    "\n",
    "#-----------------------------\n",
    "# 1. Define Optuna Objective Function\n",
    "#-----------------------------\n",
    "def objective_lgb(trial):\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'device_type': 'gpu',\n",
    "        'n_estimators': 500,  # ✅ 提高上限為 1000\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 64),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 6),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 20, 100),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.5, 10.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1.0, 10.0)\n",
    "    }\n",
    "\n",
    "    aucs = []\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    for train_idx, val_idx in skf.split(X_train, y_train):  # ✅ 避免洩漏，使用 X_train\n",
    "        X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        model = LGBMClassifier(**params)\n",
    "        model.fit(\n",
    "            X_fold_train, y_fold_train,\n",
    "            eval_set=[(X_fold_val, y_fold_val)],\n",
    "            eval_metric='auc',\n",
    "            callbacks=[lgb.early_stopping(50)]\n",
    "        )\n",
    "\n",
    "        preds = model.predict_proba(X_fold_val)[:, 1]\n",
    "        aucs.append(roc_auc_score(y_fold_val, preds))\n",
    "\n",
    "    return np.mean(aucs)\n",
    "\n",
    "#-----------------------------\n",
    "# 2. Run Optuna Optimization\n",
    "#-----------------------------\n",
    "study_lgb = optuna.create_study(direction='maximize')\n",
    "study_lgb.optimize(objective_lgb, n_trials=30)\n",
    "\n",
    "print(\"Best LightGBM Parameters:\")\n",
    "print(study_lgb.best_params)\n",
    "print(f\"Best AUC: {study_lgb.best_value:.4f}\")\n",
    "\n",
    "#-----------------------------\n",
    "# 3. Train Final Model\n",
    "#-----------------------------\n",
    "best_lgb = LGBMClassifier(\n",
    "    **study_lgb.best_params,\n",
    "    n_estimators=500,\n",
    "    verbosity=-1,\n",
    "    device_type='gpu'\n",
    ")\n",
    "\n",
    "# ✅ 最終訓練只用 X_train，評估用 X_val\n",
    "best_lgb.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_train, y_train), (X_val, y_val)],  # ✅ 加入 train\n",
    "    eval_metric='auc',\n",
    "    callbacks=[lgb.early_stopping(50)]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#-----------------------------\n",
    "# 4. Draw Learning Curve\n",
    "#-----------------------------\n",
    "results = best_lgb.evals_result_\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(results['training']['auc'], label='Train AUC')         # ✅ eval_set[0]\n",
    "plt.plot(results['valid_1']['auc'], label='Validation AUC') \n",
    "plt.xlabel('Boosting Round')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.title('LightGBM Learning Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#-----------------------------\n",
    "# 5. Predict & Confusion Matrix\n",
    "#-----------------------------\n",
    "lgb_preds = best_lgb.predict_proba(X_val)\n",
    "lgb_labels = (lgb_preds[:, 1] >= 0.5).astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_val, lgb_labels)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('LightGBM Confusion Matrix (Validation Set)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#-----------------------------\n",
    "# 6. Feature Importance\n",
    "#-----------------------------\n",
    "plt.figure(figsize=(10, 6))\n",
    "lgb.plot_importance(best_lgb, max_num_features=15, importance_type='gain', height=0.5)\n",
    "plt.title('LightGBM Feature Importance (Top 15, by Gain)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#-----------------------------\n",
    "# 7. SHAP Analysis\n",
    "#-----------------------------\n",
    "explainer = shap.Explainer(best_lgb, X_train, feature_names=X_train.columns)\n",
    "shap_values = explainer(X_val)\n",
    "shap.plots.beeswarm(shap_values, max_display=15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 CatBoost 模型優化與訓練\n",
    "\n",
    "CatBoost 是一種高效能的梯度提升樹實現，尤其擅長處理類別型特徵，並自動處理缺失值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T06:53:30.761095Z",
     "iopub.status.busy": "2025-04-15T06:53:30.760877Z",
     "iopub.status.idle": "2025-04-15T06:56:51.960402Z",
     "shell.execute_reply": "2025-04-15T06:56:51.959716Z",
     "shell.execute_reply.started": "2025-04-15T06:53:30.761078Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "#-----------------------------\n",
    "# 1. Define Optuna Objective Function (✅ 使用 X_train 進行 CV 調參)\n",
    "#-----------------------------\n",
    "def objective_cat(trial):\n",
    "    params = {\n",
    "        'iterations': 500, \n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "        'depth': trial.suggest_int('depth', 3, 6),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 3.0, 10.0),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.2, 1.0),\n",
    "        'border_count': trial.suggest_int('border_count', 64, 128),\n",
    "        'eval_metric': 'AUC',\n",
    "        'random_seed': 42,\n",
    "        'task_type': 'CPU',\n",
    "        'od_type': 'Iter',\n",
    "        'od_wait': 50,\n",
    "        'verbose': 0\n",
    "    }\n",
    "\n",
    "    aucs = []\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    for train_idx, val_idx in skf.split(X_train, y_train):  # ✅ 用 X_train 進行 5-fold CV\n",
    "        X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        train_pool = Pool(X_fold_train, y_fold_train)\n",
    "        val_pool = Pool(X_fold_val, y_fold_val)\n",
    "\n",
    "        model = CatBoostClassifier(**params)\n",
    "        model.fit(train_pool, eval_set=val_pool, use_best_model=True, verbose=False)\n",
    "\n",
    "        preds = model.predict_proba(X_fold_val)[:, 1]\n",
    "        auc = roc_auc_score(y_fold_val, preds)\n",
    "        aucs.append(auc)\n",
    "\n",
    "    return np.mean(aucs)\n",
    "\n",
    "#-----------------------------\n",
    "# 2. Run Optuna Optimization\n",
    "#-----------------------------\n",
    "study_cat = optuna.create_study(direction='maximize')\n",
    "study_cat.optimize(objective_cat, n_trials=30)\n",
    "\n",
    "print(\"Best CatBoost Parameters:\")\n",
    "print(study_cat.best_params)\n",
    "print(f\"Best AUC: {study_cat.best_value:.4f}\")\n",
    "\n",
    "#-----------------------------\n",
    "# 3. Train Final Model on X_train (✅ 不使用整個 X 以避免資料洩漏)\n",
    "#-----------------------------\n",
    "best_cat = CatBoostClassifier(\n",
    "    **study_cat.best_params,\n",
    "    iterations=500,\n",
    "    eval_metric='AUC',\n",
    "    task_type='CPU',\n",
    "    random_seed=42,\n",
    "    use_best_model=True,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "best_cat.fit(X_train, y_train, eval_set=(X_val, y_val))\n",
    "\n",
    "\n",
    "#-----------------------------\n",
    "# 4. Draw Learning Curve\n",
    "#-----------------------------\n",
    "cat_log = best_cat.get_evals_result()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(cat_log['learn']['Logloss'], label='Train Logloss')\n",
    "plt.plot(cat_log['validation']['Logloss'], label='Validation Logloss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Logloss')\n",
    "plt.title('CatBoost Learning Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#-----------------------------\n",
    "# 5. Predict & Confusion Matrix\n",
    "#-----------------------------\n",
    "cat_preds = best_cat.predict_proba(X_val)[:, 1]\n",
    "cat_labels = (cat_preds >= 0.5).astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_val, cat_labels)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('CatBoost Confusion Matrix (Validation Set)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#-----------------------------\n",
    "# 6. Feature Importance\n",
    "#-----------------------------\n",
    "plt.figure(figsize=(10, 6))\n",
    "feat_importance = best_cat.get_feature_importance(prettified=True)\n",
    "feat_importance_top = feat_importance.sort_values(by='Importances', ascending=False).head(15)\n",
    "\n",
    "plt.barh(feat_importance_top['Feature Id'], feat_importance_top['Importances'])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel('Importance')\n",
    "plt.title('CatBoost Feature Importance (Top 15)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#-----------------------------\n",
    "# 7. SHAP Summary Plot\n",
    "#-----------------------------\n",
    "explainer = shap.Explainer(best_cat)\n",
    "shap_values = explainer(X_val)\n",
    "shap.plots.beeswarm(shap_values, max_display=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 模型集成與預測\n",
    "\n",
    "在這部分，我們將根據各個模型在驗證集上的 AUC 表現，進行自動加權集成，結合三個模型的預測結果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T06:56:51.961779Z",
     "iopub.status.busy": "2025-04-15T06:56:51.961217Z",
     "iopub.status.idle": "2025-04-15T06:56:52.027523Z",
     "shell.execute_reply": "2025-04-15T06:56:52.026984Z",
     "shell.execute_reply.started": "2025-04-15T06:56:51.961751Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cat_test_preds = best_cat.predict_proba(X_test)[:, 1]\n",
    "lgb_test_preds = best_lgb.predict_proba(X_test)[:, 1]\n",
    "xgb_test_preds = best_xgb.predict_proba(X_test)[:, 1]\n",
    "final_preds = (cat_test_preds + lgb_test_preds + xgb_test_preds) / 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T06:56:52.028802Z",
     "iopub.status.busy": "2025-04-15T06:56:52.028199Z",
     "iopub.status.idle": "2025-04-15T06:56:52.076549Z",
     "shell.execute_reply": "2025-04-15T06:56:52.076046Z",
     "shell.execute_reply.started": "2025-04-15T06:56:52.028775Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# 1. 用預測結果填入 sample_submission\n",
    "sample_submission['smoking'] = final_preds\n",
    "\n",
    "# 2. 存成 timestamp 命名的副本（可備份多個版本）\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "filename = f\"submission_{timestamp}.csv\"\n",
    "\n",
    "# 3. 正式輸出給 Kaggle 平台（會自動出現下載按鈕）\n",
    "sample_submission.to_csv(\"/kaggle/working/submission.csv\", index=False)\n",
    "\n",
    "# 4. 額外備份一份有時間戳的副本（選用）\n",
    "sample_submission.to_csv(filename, index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7141863,
     "sourceId": 11402447,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
